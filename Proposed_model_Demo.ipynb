{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proposed_model_Demo.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "csXWGj9As4ka",
        "outputId": "79bbf661-a8fd-4567-f17c-c1524623e91d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        }
      },
      "source": [
        "import nltk\n",
        "import pandas as pd\n",
        "import re\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOnBxM0YtWAP",
        "outputId": "c8f569a9-5f85-44ac-cf86-b01bb6d4cfc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "filename = 'path to word vectors'\n",
        "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:252: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BygaBMrRyDrD"
      },
      "source": [
        "# Noun phrase extractor\n",
        "from textblob import TextBlob\n",
        "def noun_phrases(text):\n",
        "  np_text = []\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  for sentence in sentences:\n",
        "    blob = TextBlob(sentence)\n",
        "    np_sent = blob.noun_phrases\n",
        "    np_text.append(np_sent)\n",
        "  return np_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THRwp3VKzi0S"
      },
      "source": [
        "# Topic modeller\n",
        "def topic_model(text):\n",
        "  import nltk\n",
        "  import spacy\n",
        "  from spacy.lang.en import English\n",
        "  parser = English() # parser is imported\n",
        "  def tokenize(text):\n",
        "    lda_tokens = []\n",
        "    tokens = parser(text) # text is converted to tokens\n",
        "    for token in tokens: # lda tokens are created for each word\n",
        "      if token.orth_.isspace():\n",
        "        continue\n",
        "      elif token.like_url:\n",
        "        lda_tokens.append('URL')\n",
        "      elif token.orth_.startswith('@'):\n",
        "        lda_tokens.append('SCREEN_NAME')\n",
        "      else:\n",
        "        lda_tokens.append(token.lower_)\n",
        "    return lda_tokens\n",
        "  from nltk.corpus import wordnet as wn\n",
        "  def get_lemma(word):\n",
        "    lemma = wn.morphy(word)\n",
        "    if lemma is None:\n",
        "      return word\n",
        "    else:\n",
        "      return lemma\n",
        "  from nltk.stem.wordnet import WordNetLemmatizer\n",
        "  def get_lemma2(word):\n",
        "    return WordNetLemmatizer().lemmatize(word)\n",
        "  en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "  \n",
        "  def prepare_text_for_lda(text):\n",
        "    tokens = tokenize(text)\n",
        "    tokens = [token for token in tokens if len(token) > 4]\n",
        "    tokens = [token for token in tokens if token not in en_stop]\n",
        "    tokens = [get_lemma(token) for token in tokens]\n",
        "    return tokens\n",
        "  \n",
        "  from gensim import corpora\n",
        "  import gensim\n",
        "  import pickle\n",
        "  import random\n",
        "  text_data = []\n",
        "  data = text\n",
        "  data = nltk.sent_tokenize(data)\n",
        "  for i in data :\n",
        "    for line in i.split(\".\"):\n",
        "      tokens = prepare_text_for_lda(line)\n",
        "      text_data.append(tokens)\n",
        "    dictionary = corpora.Dictionary(text_data)\n",
        "    corpus = [dictionary.doc2bow(text)for text in text_data]\n",
        "    #corpus\n",
        "    pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
        "    dictionary.save('dictionary.gensim')\n",
        "    NUM_TOPICS = 1\n",
        "    ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=50)\n",
        "    ldamodel.save('model5.gensim')\n",
        "    topics = ldamodel.print_topics(num_words=30)\n",
        "    t = []\n",
        "    for topic in topics:\n",
        "      t.append(topic)\n",
        "    import re\n",
        "    t = re.sub(\"[^a-zA-Z]\",\".\",str(t))\n",
        "    t = re.sub(\"[.]+\",\" \",t)\n",
        "    t = nltk.word_tokenize(t)\n",
        "    return t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1Xhkb7-zlF0"
      },
      "source": [
        "def find_distance(text1,text2):\n",
        "  try:\n",
        "    value = model.distance(text1,text2)\n",
        "  except KeyError:\n",
        "    value = 1\n",
        "  return 1-value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_8zgXebzq-S"
      },
      "source": [
        "Statistical Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnfTUGZ5zyhd"
      },
      "source": [
        "import math\n",
        "def tf_isf_score(text):\n",
        "  sentences = sent_tokenize(text)\n",
        "  sent_count = len(sentences)\n",
        "  tf = dict()\n",
        "  sf = dict()\n",
        "  words_set = []\n",
        "  scores = []\n",
        "  for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    for word in words:\n",
        "      words_set.append(word)\n",
        "      if (word,sentence) in tf:\n",
        "        tf[word,sentence]+=1\n",
        "      else:\n",
        "        tf[word,sentence]=1\n",
        "  words_set = set(words_set)\n",
        "  for word in words_set:\n",
        "    sf[word]=0\n",
        "    for sentence in sentences:\n",
        "      words = word_tokenize(sentence)\n",
        "      for word2 in words:\n",
        "        if word2 == word:\n",
        "          sf[word]+=1\n",
        "          continue\n",
        "  for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    score = 0\n",
        "    for word in words:\n",
        "      score += tf[word,sentence]*(math.log(sent_count/sf[word]))\n",
        "    scores.append(score)\n",
        "  return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ra2Tw4vbzoNU"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "def position_score(text):\n",
        "  text = sent_tokenize(text)\n",
        "  sentence_count = len(text)\n",
        "  sent_score = []\n",
        "  for i in range(1,len(text)+1):\n",
        "    pos_score = i/(sentence_count)\n",
        "    if pos_score > 0.0 and pos_score <= 0.1:\n",
        "      sent_score.append(0.37/sentence_count)#17\n",
        "    elif pos_score > 0.1 and pos_score <= 0.2:\n",
        "      sent_score.append(0.23/sentence_count)\n",
        "    elif pos_score > 0.2 and pos_score <= 0.3:\n",
        "      sent_score.append(0.14/sentence_count)\n",
        "    elif pos_score > 0.3 and pos_score <= 0.4:\n",
        "      sent_score.append(0.08/sentence_count)\n",
        "    elif pos_score > 0.4 and pos_score <= 0.5:\n",
        "      sent_score.append(0.05/sentence_count)\n",
        "    elif pos_score > 0.5 and pos_score <= 0.6:\n",
        "      sent_score.append(0.04/sentence_count)\n",
        "    elif pos_score > 0.6 and pos_score <= 0.7:\n",
        "      sent_score.append(0.06/sentence_count)\n",
        "    elif pos_score > 0.7 and pos_score <= 0.8:\n",
        "      sent_score.append(0.04/sentence_count)\n",
        "    elif pos_score > 0.8 and pos_score <= 0.9:\n",
        "      sent_score.append(0.04/sentence_count)\n",
        "    elif pos_score > 0.9 and pos_score <= 1.0:\n",
        "      sent_score.append(0.15/sentence_count)\n",
        "  return sent_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldD6fZ6KzuCe",
        "outputId": "fa399df0-3224-437a-9574-33c3145ebffe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "def pos_tags(text):\n",
        "  return pos_tag(word_tokenize(text))\n",
        "def named_entity(text):\n",
        "  text = pos_tags(text)\n",
        "  return ne_chunk(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7I3NxAJ0O0r"
      },
      "source": [
        "from collections import Counter\n",
        "def noun_counter(text):\n",
        "  counts = []\n",
        "  sentences = sent_tokenize(text)\n",
        "  for sentence in sentences:\n",
        "    a = pos_tags(sentence)\n",
        "    count = Counter(tag for word,tag in a)\n",
        "    counts.append(count['NN'])\n",
        "  if len(counts)>0:\n",
        "    m = max(counts)\n",
        "  else:\n",
        "    m = 1\n",
        "  counts = [(count/m) for count in counts]\n",
        "  return counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7ql7PH_z3iS"
      },
      "source": [
        "def sent_length_score(text):\n",
        "  text = sent_tokenize(text)\n",
        "  max_len = max(len(i) for i in text)\n",
        "  sent_lengths = []\n",
        "  for sentence in text:\n",
        "    sent_lengths.append(len(sentence)/max_len)\n",
        "  return sent_lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T70_UpHJz6sS"
      },
      "source": [
        "def sent_numerical_score(text):\n",
        "  num_scores = []\n",
        "  text = sent_tokenize(text)\n",
        "  for sentence in text:\n",
        "    count = 0\n",
        "    words = word_tokenize(sentence)\n",
        "    sent_len = len(words)\n",
        "    for word in words:\n",
        "      if word.replace('.','',1).isdigit():\n",
        "        count+=1\n",
        "    num_scores.append(count/sent_len)\n",
        "  return num_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4v34GDL0Ma7"
      },
      "source": [
        "def sentiment_score(text):\n",
        "  from textblob import TextBlob\n",
        "  sentences = sent_tokenize(text)\n",
        "  scores = []\n",
        "  for sentence in sentences:\n",
        "    blob = TextBlob(text)\n",
        "    score = blob.sentiment.polarity\n",
        "    if(score<0):\n",
        "      score *=-1\n",
        "    scores.append(score)\n",
        "  return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV41P7-P0Y8k"
      },
      "source": [
        "Pre-Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVRkMPYO0ST6"
      },
      "source": [
        "def text_cleaner(text):\n",
        "  new_text = text.lower()\n",
        "  new_text = re.sub(\"[^a-zA-Z0-9 .?]\", \" \", str(text))    # removing anything other than english letters, digits, full stops and question marks\n",
        "  new_text = re.sub(\"[ +]\", \" \", new_text) # removing unwanted whitespaces\n",
        "  new_text = re.sub(\"[.]{2,}\",\"\", new_text) # removing unwanted period symbols\n",
        "  return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1WE1RQD0YRz",
        "outputId": "09b9c61b-b587-4a00-94cd-2f5d782af788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "def stopwords_remover(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  new_text = \"\"\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  for words in word_tokens:\n",
        "    if words not in stop_words:\n",
        "      new_text = new_text + words + \" \"\n",
        "  return new_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPGItaoY1YQD",
        "outputId": "16bdd2bb-2f60-4cfb-c888-895ea5a3c514",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "nltk.download('wordnet')\n",
        "def stemmer(text):\n",
        "  from nltk.stem import PorterStemmer\n",
        "  ps =PorterStemmer()\n",
        "  sentences = sent_tokenize(text)\n",
        "  new_text = \"\"\n",
        "  for sentence in sentences:\n",
        "    temp_text = \"\"\n",
        "    words = word_tokenize(sentence)\n",
        "    for word in words:\n",
        "      word = ps.stem(word)\n",
        "      temp_text = temp_text + word + \" \"\n",
        "    new_text = new_text + temp_text\n",
        "  return new_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k35Hrvpi1oLk"
      },
      "source": [
        "def new_model(text,output_len):\n",
        "\n",
        "  import warnings\n",
        "  warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  #print(\"No of sentences in input :\",len(sentences))\n",
        "\n",
        "  if(output_len==None):\n",
        "    output_len = len(sentences)//5\n",
        "  if(len(sentences)<output_len):\n",
        "    print(\"Output length is greater than input length\")\n",
        "    return\n",
        "  \n",
        "  #text = text_cleaner(text)\n",
        "  scores = []\n",
        "  s = nltk.sent_tokenize(text)\n",
        "  score1 = tf_isf_score(text)\n",
        "  score2 = position_score(text)\n",
        "  score3 = sent_length_score(text)\n",
        "  score4 = sent_numerical_score(text)\n",
        "  score5 = noun_counter(text)\n",
        "  score6 = sentiment_score(text)\n",
        "  scores_2 = []\n",
        "  for i in range(len(s)):\n",
        "    scores_2.append(score1[i]+score2[i]+score3[i]+score4[i]+score5[i]+score6[i])\n",
        "  \n",
        "  topics = topic_model(text)\n",
        "  nps = noun_phrases(text)\n",
        "  if(len(nltk.sent_tokenize(text))!=len(nps)):\n",
        "    print(\"Wrong tokenization\")\n",
        "    return\n",
        "  scores_1 = []\n",
        "  for np_sent in nps:\n",
        "    score = []\n",
        "    for np_each in np_sent:\n",
        "      score_each_np = 1\n",
        "      np_words = nltk.word_tokenize(np_each)\n",
        "      for np_each_word in np_words:\n",
        "        score_each_np_word = 0\n",
        "        for topic in topics:\n",
        "          score_each_np_word += find_distance(topic,np_each_word)\n",
        "        score_each_np *= score_each_np_word\n",
        "      score.append(score_each_np)\n",
        "    scores_1.append(sum(score))\n",
        "\n",
        "  \n",
        "  if(len(scores_1)!=len(scores_2)):\n",
        "    print(\"Diff score lengths\",len(scores_1),len(scores_2))\n",
        "    return\n",
        "  \n",
        "  for i in range(len(scores_1)):\n",
        "    scores.append(scores_1[i]+scores_2[i])\n",
        "  data_op = pd.DataFrame({'text':sentences,'scores':scores})\n",
        "  data_op = data_op.sort_values(by=['scores'],ascending=False)\n",
        "\n",
        "  x = data_op.values\n",
        "  new_text = \"\"\n",
        "  for i in range(output_len):\n",
        "    new_text += x[i][0]+\" \"\n",
        "  return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6IeyI_GV1rnF",
        "outputId": "cbd8bb38-f96d-436a-dbad-886c8eca122b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "text = \"A car (or automobile) is a wheeled motor vehicle used for transportation. Most definitions of cars say that they run primarily on roads, seat one to eight people, have four tires, and mainly transport people rather than goods.[2][3]Cars came into global use during the 20th century, and developed economies depend on them. The year 1886 is regarded as the birth year of the modern car when German inventor Karl Benz patented his Benz Patent-Motorwagen. Cars became widely available in the early 20th century. One of the first cars accessible to the masses was the 1908 Model T, an American car manufactured by the Ford Motor Company. Cars were rapidly adopted in the US, where they replaced animal-drawn carriages and carts, but took much longer to be accepted in Western Europe and other parts of the world.[citation needed]Cars have controls for driving, parking, passenger comfort, and a variety of lights. Over the decades, additional features and controls have been added to vehicles, making them progressively more complex, but also more reliable and easier to operate.[citation needed] These include rear reversing cameras, air conditioning, navigation systems, and in-car entertainment. Most cars in use in the 2010s are propelled by an internal combustion engine, fueled by the combustion of fossil fuels. Electric cars, which were invented early in the history of the car, became commercially available in the 2000s and are predicted to cost less to buy than gasoline cars before 2025.[4][5]There are costs and benefits to car use. The costs to the individual include acquiring the vehicle, interest payments (if the car is financed), repairs and maintenance, fuel, depreciation, driving time, parking fees, taxes, and insurance.[6] The costs to society include maintaining roads, land use, road congestion, air pollution, public health, health care, and disposing of the vehicle at the end of its life. Traffic collisions are the largest cause of injury-related deaths worldwide.[7]The personal benefits include on-demand transportation, mobility, independence, and convenience.[8] The societal benefits include economic benefits, such as job and wealth creation from the automotive industry, transportation provision, societal well-being from leisure and travel opportunities, and revenue generation from the taxes. People's ability to move flexibly from place to place has far-reaching implications for the nature of societies.[9] There are around 1 billion cars in use worldwide. The numbers are increasing rapidly, especially in China, India and other newly industrialized countries[10].\"\n",
        "output_sentence = None\n",
        "summary = new_model(text,output_sentence)\n",
        "print(\"No of sentences in output\",len(nltk.sent_tokenize(summary)))\n",
        "print(summary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of sentences in output 4\n",
            "[8] The societal benefits include economic benefits, such as job and wealth creation from the automotive industry, transportation provision, societal well-being from leisure and travel opportunities, and revenue generation from the taxes. Electric cars, which were invented early in the history of the car, became commercially available in the 2000s and are predicted to cost less to buy than gasoline cars before 2025. Cars were rapidly adopted in the US, where they replaced animal-drawn carriages and carts, but took much longer to be accepted in Western Europe and other parts of the world. Most definitions of cars say that they run primarily on roads, seat one to eight people, have four tires, and mainly transport people rather than goods. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ptDBE-h9Vs6",
        "outputId": "31061e9a-dcbf-47a8-93de-91c7b44c504f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "input_text = input(\"Enter the text to be summarised :\")\n",
        "output_sentence = None\n",
        "summary = new_model(input_text,output_sentence)\n",
        "print(\"No of sentences in output :\",len(nltk.sent_tokenize(summary)))\n",
        "print(summary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the text to be summarised :A car (or automobile) is a wheeled motor vehicle used for transportation. Most definitions of cars say that they run primarily on roads, seat one to eight people, have four tires, and mainly transport people rather than goods.[2][3]Cars came into global use during the 20th century, and developed economies depend on them. The year 1886 is regarded as the birth year of the modern car when German inventor Karl Benz patented his Benz Patent-Motorwagen. Cars became widely available in the early 20th century. One of the first cars accessible to the masses was the 1908 Model T, an American car manufactured by the Ford Motor Company. Cars were rapidly adopted in the US, where they replaced animal-drawn carriages and carts, but took much longer to be accepted in Western Europe and other parts of the world.[citation needed]Cars have controls for driving, parking, passenger comfort, and a variety of lights. Over the decades, additional features and controls have been added to vehicles, making them progressively more complex, but also more reliable and easier to operate.[citation needed] These include rear reversing cameras, air conditioning, navigation systems, and in-car entertainment. Most cars in use in the 2010s are propelled by an internal combustion engine, fueled by the combustion of fossil fuels. Electric cars, which were invented early in the history of the car, became commercially available in the 2000s and are predicted to cost less to buy than gasoline cars before 2025.[4][5]There are costs and benefits to car use. The costs to the individual include acquiring the vehicle, interest payments (if the car is financed), repairs and maintenance, fuel, depreciation, driving time, parking fees, taxes, and insurance.[6] The costs to society include maintaining roads, land use, road congestion, air pollution, public health, health care, and disposing of the vehicle at the end of its life. Traffic collisions are the largest cause of injury-related deaths worldwide.[7]The personal benefits include on-demand transportation, mobility, independence, and convenience.[8] The societal benefits include economic benefits, such as job and wealth creation from the automotive industry, transportation provision, societal well-being from leisure and travel opportunities, and revenue generation from the taxes. People's ability to move flexibly from place to place has far-reaching implications for the nature of societies.[9] There are around 1 billion cars in use worldwide. The numbers are increasing rapidly, especially in China, India and other newly industrialized countries[10].\n",
            "No of sentences in output : 4\n",
            "[8] The societal benefits include economic benefits, such as job and wealth creation from the automotive industry, transportation provision, societal well-being from leisure and travel opportunities, and revenue generation from the taxes. Electric cars, which were invented early in the history of the car, became commercially available in the 2000s and are predicted to cost less to buy than gasoline cars before 2025. Cars were rapidly adopted in the US, where they replaced animal-drawn carriages and carts, but took much longer to be accepted in Western Europe and other parts of the world. Most definitions of cars say that they run primarily on roads, seat one to eight people, have four tires, and mainly transport people rather than goods. \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
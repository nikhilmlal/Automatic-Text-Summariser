{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Statistical_model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgUlQwXR98Bu",
        "outputId": "99a9c5d5-7881-498b-b7af-aea32245d102",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 827
        }
      },
      "source": [
        "# importing packages\n",
        "import nltk\n",
        "import re\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "nltk.download('punkt')\n",
        "!pip install scikit-fuzzy\n",
        "!pip install sumy\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Collecting scikit-fuzzy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6c/f0/5eb5dbe0fd8dfe7d4651a8f4e591a196623a22b9e5339101e559695b4f6c/scikit-fuzzy-0.4.2.tar.gz (993kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from scikit-fuzzy) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from scikit-fuzzy) (1.4.1)\n",
            "Requirement already satisfied: networkx>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from scikit-fuzzy) (2.5)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=1.9.0->scikit-fuzzy) (4.4.2)\n",
            "Building wheels for collected packages: scikit-fuzzy\n",
            "  Building wheel for scikit-fuzzy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-fuzzy: filename=scikit_fuzzy-0.4.2-cp36-none-any.whl size=894070 sha256=10ac57f2405664f851434f07cdf34c456970d15e2736ac39cad276ac67ba60b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b9/4e/77/da79b16f64ef1738d95486e2731eea09d73e90a72465096600\n",
            "Successfully built scikit-fuzzy\n",
            "Installing collected packages: scikit-fuzzy\n",
            "Successfully installed scikit-fuzzy-0.4.2\n",
            "Collecting sumy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/61/20/8abf92617ec80a2ebaec8dc1646a790fc9656a4a4377ddb9f0cc90bc9326/sumy-0.8.1-py2.py3-none-any.whl (83kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from sumy) (2.23.0)\n",
            "Requirement already satisfied: docopt<0.7,>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from sumy) (0.6.2)\n",
            "Collecting pycountry>=18.2.23\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/73/6f1a412f14f68c273feea29a6ea9b9f1e268177d32e0e69ad6790d306312/pycountry-20.7.3.tar.gz (10.1MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1MB 7.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from sumy) (3.2.5)\n",
            "Collecting breadability>=0.1.20\n",
            "  Downloading https://files.pythonhosted.org/packages/ad/2d/bb6c9b381e6b6a432aa2ffa8f4afdb2204f1ff97cfcc0766a5b7683fec43/breadability-0.1.20.tar.gz\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->sumy) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk>=3.0.2->sumy) (1.15.0)\n",
            "Requirement already satisfied: lxml>=2.0 in /usr/local/lib/python3.6/dist-packages (from breadability>=0.1.20->sumy) (4.2.6)\n",
            "Building wheels for collected packages: pycountry, breadability\n",
            "  Building wheel for pycountry (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycountry: filename=pycountry-20.7.3-py2.py3-none-any.whl size=10746865 sha256=e1fde9d351b81c9cce7dc3b84b28b581f61c522435bdaf6a47497c2b6023e812\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/4e/a6/be297e6b83567e537bed9df4a93f8590ec01c1acfbcd405348\n",
            "  Building wheel for breadability (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for breadability: filename=breadability-0.1.20-py2.py3-none-any.whl size=21681 sha256=9acc716f952272c13302aed0b6d18eff9e1346dc96d1e1d6e4677ba7b93b26bd\n",
            "  Stored in directory: /root/.cache/pip/wheels/5a/4d/a1/510b12c5e65e0b2b3ce539b2af66da0fc57571e528924f4a52\n",
            "Successfully built pycountry breadability\n",
            "Installing collected packages: pycountry, breadability, sumy\n",
            "Successfully installed breadability-0.1.20 pycountry-20.7.3 sumy-0.8.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gpi55JS-caU"
      },
      "source": [
        "# applying text cleaning\n",
        "def text_cleaner(text):\n",
        "  new_text = text.lower()\n",
        "  new_text = re.sub(\"[^a-zA-Z0-9 .?]\", \" \", str(text))    # removing anything other than english letters, digits, period symbols and question marks\n",
        "  new_text = re.sub(\"[ +]\", \" \", new_text) # removing unwanted whitespace\n",
        "  new_text = re.sub(\"[.]{2,}\",\".\", new_text) # removing unwanted period symbols\n",
        "  return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7Q_09oK-iFZ",
        "outputId": "eb87d6f5-0773-462d-837c-fc22f34779a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# removing stopwords\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "def stopwords_remover(text):\n",
        "  word_tokens = word_tokenize(text)\n",
        "  new_text = \"\"\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  for words in word_tokens:\n",
        "    if words not in stop_words:\n",
        "      new_text = new_text + words + \" \"\n",
        "  return new_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6lYbLoz-j17",
        "outputId": "4d420d83-38b2-41ba-b056-204c165665e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# applying lemmatisation (not used in program)\n",
        "nltk.download('wordnet')\n",
        "def lemmatiser(text):\n",
        "  from nltk.stem import WordNetLemmatizer\n",
        "  l = WordNetLemmatizer()\n",
        "  return l.lemmatize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NsAQE0og-mTh",
        "outputId": "28643e83-8317-4878-9687-ccb239646dfe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# applying stemming\n",
        "nltk.download('wordnet')\n",
        "def stemmer(text):\n",
        "  from nltk.stem import PorterStemmer\n",
        "  ps =PorterStemmer()\n",
        "  sentences = sent_tokenize(text)\n",
        "  new_text = \"\"\n",
        "  for sentence in sentences:\n",
        "    temp_text = \"\"\n",
        "    words = word_tokenize(sentence)\n",
        "    for word in words:\n",
        "      word = ps.stem(word)\n",
        "      temp_text = temp_text + word +\" \"\n",
        "    new_text = new_text + temp_text\n",
        "  return new_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNw7Uthw-zwo"
      },
      "source": [
        "import math\n",
        "def tf_isf_score(text):\n",
        "  sentences = sent_tokenize(text)\n",
        "  sent_count = len(sentences)\n",
        "  tf = dict()\n",
        "  sf = dict()\n",
        "  words_set = []\n",
        "  scores = []\n",
        "  for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    for word in words:\n",
        "      words_set.append(word)\n",
        "      if (word,sentence) in tf:\n",
        "        tf[word,sentence]+=1 # for each word, tf value is calculated \n",
        "      else:\n",
        "        tf[word,sentence]=1\n",
        "  words_set = set(words_set)\n",
        "  for word in words_set:\n",
        "    sf[word]=0\n",
        "    for sentence in sentences:\n",
        "      words = word_tokenize(sentence)\n",
        "      for word2 in words:\n",
        "        if word2 == word:\n",
        "          sf[word]+=1\n",
        "          continue\n",
        "  for sentence in sentences:\n",
        "    words = word_tokenize(sentence)\n",
        "    score = 0\n",
        "    for word in words:\n",
        "      score += tf[word,sentence]*(math.log(sent_count/sf[word])) # TF-ISF score is calculated and returned\n",
        "    scores.append(score)\n",
        "  return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05Z33Iyp-nmX"
      },
      "source": [
        "# returns position score\n",
        "from nltk.tokenize import sent_tokenize\n",
        "def position_score(text):\n",
        "  text = sent_tokenize(text)\n",
        "  sentence_count = len(text)\n",
        "  sent_score = []\n",
        "  for i in range(1,len(text)+1):\n",
        "    pos_score = i/(sentence_count)\n",
        "    if pos_score > 0.0 and pos_score <= 0.1:\n",
        "      sent_score.append(0.17/sentence_count)\n",
        "    elif pos_score > 0.1 and pos_score <= 0.2:\n",
        "      sent_score.append(0.23/sentence_count)\n",
        "    elif pos_score > 0.2 and pos_score <= 0.3:\n",
        "      sent_score.append(0.14/sentence_count)\n",
        "    elif pos_score > 0.3 and pos_score <= 0.4:\n",
        "      sent_score.append(0.08/sentence_count)\n",
        "    elif pos_score > 0.4 and pos_score <= 0.5:\n",
        "      sent_score.append(0.05/sentence_count)\n",
        "    elif pos_score > 0.5 and pos_score <= 0.6:\n",
        "      sent_score.append(0.04/sentence_count)\n",
        "    elif pos_score > 0.6 and pos_score <= 0.7:\n",
        "      sent_score.append(0.06/sentence_count)\n",
        "    elif pos_score > 0.7 and pos_score <= 0.8:\n",
        "      sent_score.append(0.04/sentence_count)\n",
        "    elif pos_score > 0.8 and pos_score <= 0.9:\n",
        "      sent_score.append(0.04/sentence_count)\n",
        "    elif pos_score > 0.9 and pos_score <= 1.0:\n",
        "      sent_score.append(0.15/sentence_count)\n",
        "  return sent_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ny72igsT-xq7",
        "outputId": "158b71e4-3fe8-43da-cddb-83a6b75a8cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "# returns part-of-speeches\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "def pos_tags(text):\n",
        "  return pos_tag(word_tokenize(text))\n",
        "def named_entity(text):\n",
        "  text = pos_tags(text)\n",
        "  return ne_chunk(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4SlymEV-3Yj"
      },
      "source": [
        "# returns length of sentence\n",
        "def sent_length_score(text):\n",
        "  text = sent_tokenize(text)\n",
        "  max_len = max(len(i) for i in text)\n",
        "  sent_lengths = []\n",
        "  for sentence in text:\n",
        "    sent_lengths.append(len(sentence)/max_len)\n",
        "  return sent_lengths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWfwZLGD-42Q"
      },
      "source": [
        "# return count of digits\n",
        "def sent_numerical_score(text):\n",
        "  num_scores = []\n",
        "  text = sent_tokenize(text)\n",
        "  for sentence in text:\n",
        "    count = 0\n",
        "    words = word_tokenize(sentence)\n",
        "    sent_len = len(words)\n",
        "    for word in words:\n",
        "      if word.replace('.','',1).isdigit():\n",
        "        count+=1\n",
        "    num_scores.append(count/sent_len)\n",
        "  return num_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYuUB9Af-6zk"
      },
      "source": [
        "# return count of nouns in the sentence\n",
        "from collections import Counter\n",
        "def noun_counter(text):\n",
        "  counts = []\n",
        "  sentences = sent_tokenize(text)\n",
        "  for sentence in sentences:\n",
        "    a = pos_tags(sentence)\n",
        "    count = Counter(tag for word,tag in a)\n",
        "    counts.append(count['NN'])\n",
        "  if len(counts)>0:\n",
        "    m = max(counts)\n",
        "  else:\n",
        "    m = 0\n",
        "  counts = [(count/m) for count in counts]\n",
        "  return counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dkv7t6g7-9An"
      },
      "source": [
        "# returns sentiment polarity\n",
        "def sentiment_score(text):\n",
        "  from textblob import TextBlob\n",
        "  sentences = sent_tokenize(text)\n",
        "  scores = []\n",
        "  for sentence in sentences:\n",
        "    blob = TextBlob(text)\n",
        "    score = blob.sentiment.polarity\n",
        "    if(score<0):\n",
        "      score *=-1\n",
        "    scores.append(score)\n",
        "  return scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HTDLG6O--xh"
      },
      "source": [
        "def statistical_method(text,op_len):\n",
        "  s = nltk.sent_tokenize(text)\n",
        "  #print(\"Number of sentences in input :\",len(s))\n",
        "  if(op_len==None):\n",
        "    op_len = len(s)//5\n",
        "  scores = []\n",
        "  #text1 = stopwords_remover(text)\n",
        "  #text1 = stemmer(text1)\n",
        "  #text1 = text_cleaner(text1)\n",
        "  text1 = text\n",
        "  sentences = nltk.sent_tokenize(text1) \n",
        "  score1 = (tf_isf_score(text1)) # for each sentence these 6 scores are calculated\n",
        "  score2 = (position_score(text1))\n",
        "  score3 = (sent_length_score(text1))\n",
        "  score4 = (sent_numerical_score(text1))\n",
        "  score5 = (noun_counter(text1))\n",
        "  score6 = (sentiment_score(text1))\n",
        "  '''print(len(score1))\n",
        "  print(len(score2))\n",
        "  print(len(score3))\n",
        "  print(len(score4))\n",
        "  print(len(score5))\n",
        "  print(len(score6))'''\n",
        "  for i in range(len(sentences)):\n",
        "    scores.append(score1[i]+score2[i]+score3[i]+score4[i]+score5[i]+score6[i]) #the scores are summed\n",
        "  data_op = pd.DataFrame({'text':s,'scores':scores})\n",
        "  data_op = data_op.sort_values(by=['scores'],ascending=False) # scores are sorted\n",
        "  data = data_op.values\n",
        "  new_text = \"\"\n",
        "  for i in range(op_len):\n",
        "    new_text += data[i][0]+\" \"\n",
        "  return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwrDdEFu_D1y",
        "outputId": "7a14f426-c676-437f-ee85-6d95438e7e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "text = \"A car (or automobile) is a wheeled motor vehicle used for transportation. Most definitions of cars say that they run primarily on roads, seat one to eight people, have four tires, and mainly transport people rather than goods.[2][3]Cars came into global use during the 20th century, and developed economies depend on them. The year 1886 is regarded as the birth year of the modern car when German inventor Karl Benz patented his Benz Patent-Motorwagen. Cars became widely available in the early 20th century. One of the first cars accessible to the masses was the 1908 Model T, an American car manufactured by the Ford Motor Company. Cars were rapidly adopted in the US, where they replaced animal-drawn carriages and carts, but took much longer to be accepted in Western Europe and other parts of the world.[citation needed]Cars have controls for driving, parking, passenger comfort, and a variety of lights. Over the decades, additional features and controls have been added to vehicles, making them progressively more complex, but also more reliable and easier to operate.[citation needed] These include rear reversing cameras, air conditioning, navigation systems, and in-car entertainment. Most cars in use in the 2010s are propelled by an internal combustion engine, fueled by the combustion of fossil fuels. Electric cars, which were invented early in the history of the car, became commercially available in the 2000s and are predicted to cost less to buy than gasoline cars before 2025.[4][5]There are costs and benefits to car use. The costs to the individual include acquiring the vehicle, interest payments (if the car is financed), repairs and maintenance, fuel, depreciation, driving time, parking fees, taxes, and insurance.[6] The costs to society include maintaining roads, land use, road congestion, air pollution, public health, health care, and disposing of the vehicle at the end of its life. Traffic collisions are the largest cause of injury-related deaths worldwide.[7]The personal benefits include on-demand transportation, mobility, independence, and convenience.[8] The societal benefits include economic benefits, such as job and wealth creation from the automotive industry, transportation provision, societal well-being from leisure and travel opportunities, and revenue generation from the taxes. People's ability to move flexibly from place to place has far-reaching implications for the nature of societies.[9] There are around 1 billion cars in use worldwide. The numbers are increasing rapidly, especially in China, India and other newly industrialized countries[10].\"\n",
        "output_sentence = None\n",
        "summary = statistical_method(text,output_sentence)\n",
        "print(\"No of sentences in output :\",len(nltk.sent_tokenize(summary)))\n",
        "print(summary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No of sentences in output : 4\n",
            "[8] The societal benefits include economic benefits, such as job and wealth creation from the automotive industry, transportation provision, societal well-being from leisure and travel opportunities, and revenue generation from the taxes. Cars were rapidly adopted in the US, where they replaced animal-drawn carriages and carts, but took much longer to be accepted in Western Europe and other parts of the world. Most definitions of cars say that they run primarily on roads, seat one to eight people, have four tires, and mainly transport people rather than goods. Electric cars, which were invented early in the history of the car, became commercially available in the 2000s and are predicted to cost less to buy than gasoline cars before 2025. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrL_bJ-oVBQJ",
        "outputId": "221ba5e0-96d6-4d50-b1ab-ba70027545c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "source": [
        "input_text = input(\"Enter the text to be summarised :\")\n",
        "output_sentence = None\n",
        "summary = statistical_method(input_text,output_sentence)\n",
        "print(\"No of sentences in output :\",len(nltk.sent_tokenize(summary)))\n",
        "print(summary)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the text to be summarised :A car (or automobile) is a wheeled motor vehicle used for transportation. Most definitions of cars say that they run primarily on roads, seat one to eight people, have four tires, and mainly transport people rather than goods.[2][3]Cars came into global use during the 20th century, and developed economies depend on them. The year 1886 is regarded as the birth year of the modern car when German inventor Karl Benz patented his Benz Patent-Motorwagen. Cars became widely available in the early 20th century. One of the first cars accessible to the masses was the 1908 Model T, an American car manufactured by the Ford Motor Company. Cars were rapidly adopted in the US, where they replaced animal-drawn carriages and carts, but took much longer to be accepted in Western Europe and other parts of the world.[citation needed]Cars have controls for driving, parking, passenger comfort, and a variety of lights. Over the decades, additional features and controls have been added to vehicles, making them progressively more complex, but also more reliable and easier to operate.[citation needed] These include rear reversing cameras, air conditioning, navigation systems, and in-car entertainment. Most cars in use in the 2010s are propelled by an internal combustion engine, fueled by the combustion of fossil fuels. Electric cars, which were invented early in the history of the car, became commercially available in the 2000s and are predicted to cost less to buy than gasoline cars before 2025.[4][5]There are costs and benefits to car use. The costs to the individual include acquiring the vehicle, interest payments (if the car is financed), repairs and maintenance, fuel, depreciation, driving time, parking fees, taxes, and insurance.[6] The costs to society include maintaining roads, land use, road congestion, air pollution, public health, health care, and disposing of the vehicle at the end of its life. Traffic collisions are the largest cause of injury-related deaths worldwide.[7]The personal benefits include on-demand transportation, mobility, independence, and convenience.[8] The societal benefits include economic benefits, such as job and wealth creation from the automotive industry, transportation provision, societal well-being from leisure and travel opportunities, and revenue generation from the taxes. People's ability to move flexibly from place to place has far-reaching implications for the nature of societies.[9] There are around 1 billion cars in use worldwide. The numbers are increasing rapidly, especially in China, India and other newly industrialized countries[10].\n",
            "No of sentences in output : 4\n",
            "[8] The societal benefits include economic benefits, such as job and wealth creation from the automotive industry, transportation provision, societal well-being from leisure and travel opportunities, and revenue generation from the taxes. Cars were rapidly adopted in the US, where they replaced animal-drawn carriages and carts, but took much longer to be accepted in Western Europe and other parts of the world. Most definitions of cars say that they run primarily on roads, seat one to eight people, have four tires, and mainly transport people rather than goods. Electric cars, which were invented early in the history of the car, became commercially available in the 2000s and are predicted to cost less to buy than gasoline cars before 2025. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zXkLExo2n5x",
        "outputId": "87074bf4-a94e-4341-879e-65aa911231db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ePlW8Sz2pPR"
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# 2. Load a file by ID and create local file.\n",
        "download1 = drive.CreateFile({'id':'1h6-YHUR8j8Bg7uPuZ8Cx9fvrV7MhDEne'}) # replace fileid with Id of file you want to access\n",
        "download1.GetContentFile('short_texts_sumy.csv') # now you can use export.csv \n",
        "\n",
        "download2 = drive.CreateFile({'id':'16twwkXh4CBk0gdcgEhxmNvDblGnKADr0'})\n",
        "download2.GetContentFile('long_texts_sumy.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-hTx_zv3jwL"
      },
      "source": [
        "shorttexts = pd.read_csv('short_texts_sumy.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wGqDwnC3yQk"
      },
      "source": [
        "short_st_summaries = []\n",
        "for i in shorttexts['text'][:200]:\n",
        "  sl = len(nltk.sent_tokenize(i))\n",
        "  short_st_summaries.append(statistical_method(i,sl//5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9Yp_R3C5TEv",
        "outputId": "73c43003-7f45-4917-82a5-51916f0e00c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(short_st_summaries)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qaHias3jpT9a"
      },
      "source": [
        "for i in range(len(short_st_summaries)-1,len(shorttexts)-1):\n",
        "  short_st_summaries.append(None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mt39irwAriM"
      },
      "source": [
        "short_texts_sumy_st = pd.DataFrame({'text':shorttexts['text'],'sumy':shorttexts['sumy'],'statistical':short_st_summaries})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjPjBR0DBJcl"
      },
      "source": [
        "longtexts = pd.read_csv('long_texts_sumy.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etmiCcefBMQr"
      },
      "source": [
        "long_st_summaries = []\n",
        "for i in longtexts['text'][:100]:\n",
        "  sl = len(nltk.sent_tokenize(i))\n",
        "  long_st_summaries.append(statistical_method(i,sl//5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uohgmUXTBmjV",
        "outputId": "b4b8d85b-68b1-4846-8f7c-4494e0684b8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(long_st_summaries)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xm2p-B83v4-l"
      },
      "source": [
        "for i in range(len(long_st_summaries)-1,len(longtexts)-1):\n",
        "  long_st_summaries.append(None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gO2VemgRyqHI"
      },
      "source": [
        "long_texts_sumy_st = pd.DataFrame({'text':longtexts['text'],'sumy':longtexts['sumy'],'statistical':long_st_summaries})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8D5qH4uny8i7"
      },
      "source": [
        "short_texts_sumy_st.to_csv('short_texts_sumy_st.csv')\n",
        "long_texts_sumy_st.to_csv('long_texts_sumy_st.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCLtptjm1SO6",
        "outputId": "36eb62f6-4b89-4880-f2b2-9f3ac587a1de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC7z0lpK1z0L"
      },
      "source": [
        "!cp short_texts_sumy_st.csv /content/gdrive/My\\ Drive/ATS/\n",
        "!cp long_texts_sumy_st.csv /content/gdrive/My\\ Drive/ATS/"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}